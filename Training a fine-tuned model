#Training a fine-tuned model  based on BERT for a specific task using HSGadviser datset
# Fine-tune BERT for question answering
model = BertForQuestionAnswering.from_pretrained("bert-base-uncased")
optimizer = AdamW(model.parameters(), lr=5e-5)

# Define dataloader
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)
for batch in tqdm(eval_dataloader, desc='Evaluating', leave=False):
    with torch.no_grad():  # Disable gradient calculation during evaluation
        input_ids = batch["input_ids"]
        attention_mask = batch["attention_mask"]
        start_positions = batch["start_positions"]
        end_positions = batch["end_positions"]
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)
        eval_loss = outputs.loss
        total_eval_loss += eval_loss.item()

# Calculate average evaluation loss
avg_eval_loss = total_eval_loss / len(eval_dataloader)
print(f'Average Evaluation Loss: {avg_eval_loss:.4f}')
